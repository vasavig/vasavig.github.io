<html>
<head>
<title>Face Detection Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Vasavi Gajarla</h1>
</div>
</div>
<div class="container">

<h2> Project 5 / Face Detection with a Sliding Window</h2>

<p>This report discusses techniques for detecting faces in images. It starts off by discussing how a classification model can be trained with both positive (images which contain faces) and negative (images which do not contain faces) instances. It then goes on to talk about how the detections are run for on the test data. This report also discusses how hard negative mining will affect the performance of a classifier.</p>

<p> In addition to the above, an experiment on cascading the SVM with KNN has been featured. Results pertaining to using LFW dataset (http://vis-www.cs.umass.edu/lfw/) have also been added. A few examples of images with bounding boxes after classification are shown. Accuracy values, Precision-recall curves are shown wherever necessary.</p>

<div style="clear:both">
<h3>HOG descriptors for positive and negative training data</h3>
<p>HOG or Histogram of Gradients are features which are used in object detection. These are found by dividing image into cells with one histogram in each. A template containing a certain number of cells is considered to be the window in which the features are matched for. In this regard, the parameter HOG cell size is very important. Since the HOGs are not overlapping, a smaller cell size will result in a denser feature space, which helps in improving accuracy (becomes evident in last section of this report), however it is computationally intensive. Some sample HOG features are visualized below:</p>
<p>The negative training data is accumulated over multiple scales from within the given limited number of images. This is an advantage because since the image is known to not have a face, any downsampled image will also not have one. This made it easier for data accumulation. It has been observed that the lower the scale factor of downsampling, the better the quality of data since denser features are gathered. However this is again a computationally expensive operation. Also there is a chance of blowing up the training data with negative instances. Therefore, a cap is put on total number of negative samples (around 20K) and features are sampled using random permutation of cells. </p>
<div style="clear:both">
<h3>SVM training and Sliding Window Detector</h3>
<p>Once the features are found, it is time to train the classifier. Choosing a LAMBDA value of  0.00001, SVM is trained. The distance from and orientation of features with respect to SVM Hyperplane tells the label of a particular instance. This measure is known as the 'Confidence', the higher the confidence the stronger is the chance of detection being a true positive. However, in order not to miss the detections a low threshold is recommended to be chosen - for example, -0.1. HOG features are extracted from each image at multiple scales and whichever has the SVM confidence higher than the threshold is added as a potential candidate for face. Per image, all these detections are sent for non-maximum supression so that overlapping bounding boxes are retained as only one of those (>1 bounding boxes with >50% overlap are anyway pointing to nearly same area).</p>
<p>Hog feature visualization, Precision Recall curve and detections on some images have been shown below:</p>

<img src="first_hog.jpg" style="float: middle; width: 50%; margin-bottom: 1em;">
<img src="avg_pres_-0.1-0.8.jpg" style="float: middle; width: 50%; height: 50%; margin-bottom: 1em;">
<img src="detections_Argentina.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_aerosmith-double.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_bksomels.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">

<div style="clear:both">

<h3>Grad Credit</h3>
<h4>Hard Negative Mining</h4>
<p>Hard negative mining is a technique which is used to add negative instances which are sure to be non-faces to the training data. The idea is to reduce the number of false positives, there by increasing precision. To do this, non-face dataset is passed to the sliding window detector. If any of the feature template in any of these images crosses the minimum threshold, this was going to be a false positive - but we know for a fact that there's no way for these images to contain a face.Therefore, these are explicitly gathered as negative instances and are later used in re-training the SVM. The results came out to be around 81% accuracy. As a lot of random negative examples have already been used in training the SVM already, this addition hasn't helped much. However, since the number of positive examples stayed the same, this training data might have been prone to overfitting to negative instances and hence the reduction in accuracy.</p>
<p>Results can be seen below:</p>
<img src="average_precision_hard.png" style="float: middle; width: 50%; height: 50%; margin-bottom: 1em;">
<img src="detections_bttf206.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_class57.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_seinfeld.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">

<h3>Extra Credit</h3>
<h4>Results of flipping Positive and Negative training examples</h4>
<div style="clear:both" >
<p>To increase the number of positive and negative examples, a technique of flipping the image and adding the corresposnding HOG features to training data  can be used. This is nothing but the flipped structure of HOG feature itself. With this technique, the CalTech data gave ~13K features and Random negative data gave ~38K features. Using this data and a HOG cell size of 3, gave rise to the best accuracy with SVM classification - 91%. This is expected because the relative number of positive examples has now increased compared to the base implementation. The results can be seen below:</p>
<img src="average_precision_best.png" style="float: middle; width: 50%; height: 50%; margin-bottom: 1em;">
<img src="detections_tori-live3.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_tp-reza-girosi.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_USA.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">
<img src="detections_wxm.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">

<p>Detections on extra test data with these settings:</p>
<img src="detections_cs143_2011_class_easy.png" style="float: left width: 20%; height: 50% margin-bottom: 1em; margin-right: 100em;">
<img src="detections_cs143_2011_class_hard.png" style="float: left width: 20%; height: 50% margin-bottom: 1em;">
<img src="detections_cs143_2013_class_easy_01.png" style="float: left width: 30%; height: 50% margin-bottom: 1em;">
<img src="detections_cs143_2013_class_easy_02.png" style="float: middle width: 50%; height: 50% margin-bottom: 1em;">

<h4>Additional KNN Classification</h4>
<div style="clear:both" >
<p>As an experiment, KNN classifier has been added to the already trained SVM model for classification. The idea is to further filter the detections based on a second classifier. The results of SVM are basically those detections which fall above a certain threshold. Once this threshold is crossed, the feature vector of this detection is checked for its 5 nearest neighbors in the training data. The detection is passed on to non-max supression and potentially as a bounding box later on only if the majority of nearest neighbors label it as a positive instance. The performance of this ensemble classifier can be seen in below figures:</p>
<p>Results:</p>
</div>
<h4>Results with LFW dataset of positive training examples</h4>
<div style="clear:both" >
<p>In order to test how the classification changes with additional positive training data, a survey dataset called 'Labeled Faces in the Wild' has been used. This dataset contains images which have been adjusted for alignment, using a technique called as 'Deep Funneling'. It is an unsupervised alginment of real-world, complex images such as factors such as pose, etc do not affect a face detection algorithm. A few example images can be seen below:</p>

<p>Adding this data to training led to following results:</p>

</div>

</body>
</html>
